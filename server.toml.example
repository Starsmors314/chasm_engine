listen = "tcp://*:25566"
world = "worlds/generic-world-name"

# params passed to the model are defined in their own section below
provider = "local"
# context_length is the context window of the model.
context_length = 3700

# max_tokens limits the text generation
max_tokens = 250

loglevel = "warning"

# There is no good reason not to use the default embedding.
# If you change embedding provider, the embedding of your previous data will be unreadable
# Leaving unset will use Sentence Transformers,
# which uses pytorch, so will use CUDA if available.
# For local, it depends, but all-MiniLM-L6-v2 is probably sufficient (the default)
# For OpenAI, you must use text-embedding-ada-002
# If using ooba, the embedding model is set by environment variable when starting the ooba API.
#embedding_provider = "OpenAI"
#embedding = "text-embedding-ada-002"

# The engine will round-robin over available providers.

[providers.OpenAI]
model = "gpt-3.5-turbo"
api_base = "https://api.openai.com/v1"
api_key = "sk-your-super-secret-key"

[providers.local]
# Oobabooga's text-generation-webui works well using the "openai" extension
# and setting the base url appropriately. Exllama is fast.
# I've been using Guanaco and Wizard-Vicuna-Uncensored, both 30B 4bit, with good results.
# I've tried Nous-Hermes 13B and it seems to be OK
# Lazarus 8k is patchy for instruction following, but does work.
# Guanaco 65B is very good.
api_base = "http://localhost:5001"
api_key = "N/A"
stop = ["</s>", "Assistant:"]
