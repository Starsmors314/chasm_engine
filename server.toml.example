listen = "tcp://*:25566"
world = "worlds/generic-world-name"

# params passed to the model are defined in their own section below
provider = "local"
# context_length is the context window of the model.
context_length = 3000

# max_tokens limits the text generation
max_tokens = 200

loglevel = "warning"

[memory]
# If you change embedding provider, the embedding of your previous data will be unreadable
# This uses pytorch, so will use CUDA if available.
# For local, it depends, but all-MiniLM-L6-v2 is probably sufficient
# For OpenAI, use text-embedding-ada-002
# If using ooba, the embedding model is set by environment variable when starting the API.
embedding_provider = "OpenAI"
embedding = "text-embedding-ada-002"

[providers.OpenAI]
model = "gpt-3.5-turbo"
api_base = "https://api.openai.com/v1"
api_key = "sk-your-super-secret-key"

[providers.local]
# Oobabooga's text-generation-webui works well using the "openai" extension
# and setting the base url appropriately. Exllama is fast.
# I've been using Guanaco and Wizard-Vicuna-Uncensored, both 30B 4bit, with good results.
# I've tried Nous-Hermes 13B and it seems to be OK
# Lazarus 8k is patchy for instruction following, but does work.
# Guanaco 65B is very good.
api_base = "http://localhost:5001"
api_key = "N/A"
stop = ["</s>", "Assistant:"]
# truncation_length can be set in the gui, I think
